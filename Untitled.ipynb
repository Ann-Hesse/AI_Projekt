{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef8d32f-6427-43e8-ba09-38c57f7851b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 13:11:12.288385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736428272.309159   14707 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736428272.315591   14707 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-09 13:11:12.341511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "##Laden aller m√∂glichen Bibliotheken##\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from textblob import TextBlob\n",
    "import gensim.downloader as api\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83668958-7d0d-45cc-a465-0674fc64caff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/ilhamfp31/yelp-review-dataset/versions/2\n",
      "Training Data:\n",
      "        Label                                               Text\n",
      "34566       0  This place is one of my favorite comic shops. ...\n",
      "223092      1  The wait time for an appointment is ridiculous...\n",
      "110270      1  I did not like this hotel at all. It's very ol...\n",
      "365013      0  Mill Avenue has a serious issue with parking. ...\n",
      "311625      0  Favorite sushi place in NV!  Price is reasonab...\n",
      "Testing Data:\n",
      "       Label                                               Text\n",
      "28631      1  I expected the prices of the entrees to be a l...\n",
      "9134       1  Review of Buffet:\\n\\nUGH!  It was very very un...\n",
      "26464      1  If you value your life, don't go to Banner Bos...\n",
      "9477       0  Before getting started, I'd like to point out ...\n",
      "5954       1  Very disappointed in this salon. Set an appt 4...\n"
     ]
    }
   ],
   "source": [
    "# Datenset aus Kagglehub laden\n",
    "path = kagglehub.dataset_download(\"ilhamfp31/yelp-review-dataset\")\n",
    "#Test- und Trainingdatenset getrennt speichern\n",
    "print(\"Path to dataset files:\", path)\n",
    "train_file_path = \"/home/jovyan/.cache/kagglehub/datasets/ilhamfp31/yelp-review-dataset/versions/2/yelp_review_polarity_csv/train.csv\"\n",
    "test_file_path = \"/home/jovyan/.cache/kagglehub/datasets/ilhamfp31/yelp-review-dataset/versions/2/yelp_review_polarity_csv/test.csv\"\n",
    "\n",
    "# Test- und Trainingsdaten in ein Pandadataframe laden\n",
    "train_df = pd.read_csv(train_file_path, header=None, names=[\"Label\", \"Text\"])  # Add column names\n",
    "test_df = pd.read_csv(test_file_path, header=None, names=[\"Label\", \"Text\"])    # Add column names\n",
    "\n",
    "# Damit die Daten schneller laden, werden nur 30% der Training und Testdaten gespeichert\n",
    "train_df = train_df.sample(frac=0.3, random_state=42)  # 30% Trainingdaten\n",
    "test_df = test_df.sample(frac=0.3, random_state=42)    # 30% Testdaten\n",
    "\n",
    "# Label f√ºr einen besseren gebrauch von 1|2 zu 0|1\n",
    "train_df['Label'] = train_df['Label'].map({2: 0, 1: 1})\n",
    "test_df['Label'] = test_df['Label'].map({2: 0, 1: 1})\n",
    "\n",
    "# Trainingsdaten und Testdaten anzeigen lassen\n",
    "print(\"Training Data:\")\n",
    "print(train_df.head())\n",
    "print(\"Testing Data:\")\n",
    "print(test_df.head())\n",
    "# DataFrames in Hugging Face Datasetformat unwandel f√ºr allf√§llige Huggingface modelle \n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7290967f-46e3-4539-8aa7-8ba13e911292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsgenauigkeit: 0.3155\n",
      "Testgenauigkeit: 0.3175\n",
      "Klassifikationsbericht f√ºr den Testdatensatz:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.03      0.05      5685\n",
      "           1       0.38      0.60      0.47      5715\n",
      "\n",
      "    accuracy                           0.32     11400\n",
      "   macro avg       0.23      0.32      0.26     11400\n",
      "weighted avg       0.23      0.32      0.26     11400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Klassifizierung von Text mit der TextBlob-Sentimentanalyse\n",
    "def textblob_sentiment(text):\n",
    "    # Bestimme den Polarit√§tswert: Positiv > 0, Negativ < 0\n",
    "    # Der Polarit√§tswert kann nicht-neutralen Text (Polarity=0) ignorieren. \n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    return 1 if polarity > 0 else 0  # Positiv wird mit 1 markiert, Negativ mit 0\n",
    "    # Es k√∂nnen auch drei Klassen (positiv, negativ, neutral) definiert werden.\n",
    "\n",
    "# Anwenden der TextBlob-Sentimentklassifizierung auf die Train- und Testdaten\n",
    "train_df['predicted_label'] = train_df['Text'].apply(textblob_sentiment)\n",
    "test_df['predicted_label'] = test_df['Text'].apply(textblob_sentiment)\n",
    "# TextBlob unterst√ºtzt keine Kontexte. \n",
    "\n",
    "# Bewertung der Genauigkeit \n",
    "train_accuracy_textblob = accuracy_score(train_df['Label'], train_df['predicted_label'])\n",
    "test_accuracy_textblob = accuracy_score(test_df['Label'], test_df['predicted_label'])\n",
    "#Ausgabe der Genauigkeit\n",
    "print(f\"Trainingsgenauigkeit: {train_accuracy_textblob:.4f}\")\n",
    "print(f\"Testgenauigkeit: {test_accuracy_textblob:.4f}\")\n",
    "\n",
    "# Klassifikationsbericht f√ºr den Testdatensatz ausgeben\n",
    "print(\"Klassifikationsbericht f√ºr den Testdatensatz:\")\n",
    "print(classification_report(test_df['Label'], test_df['predicted_label']))\n",
    "# Verbesserungsvorschlag: Bei niedriger Pr√§zision/Recall, √ºberpr√ºfe Label-Konsistenz oder Feature-Ingenieurleistung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad37a3e-93d3-4e21-9f40-2d49ff1063f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsgenauigkeit: 0.8325\n",
      "Testgenauigkeit: 0.8318\n",
      "Klassifikationsbericht f√ºr den Testdatensatz:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.83      5685\n",
      "           1       0.82      0.85      0.84      5715\n",
      "\n",
      "    accuracy                           0.83     11400\n",
      "   macro avg       0.83      0.83      0.83     11400\n",
      "weighted avg       0.83      0.83      0.83     11400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vortrainierte Wortvektoren \n",
    "word_vectors = api.load(\"glove-wiki-gigaword-300\") \n",
    "\n",
    "# Funktion, um den durchschnittlichen Wortvektor f√ºr ein Dokument zu berechnen\n",
    "def get_average_word_vector(text, word_vectors):\n",
    "    words = text.split()  # Text in einzelne W√∂rter aufteilen\n",
    "    word_vectors_list = []\n",
    "    for word in words:\n",
    "        if word in word_vectors:  # √úberpr√ºfe, ob das Wort im Vektormodell existiert\n",
    "            word_vectors_list.append(word_vectors[word])  # F√ºge den Wortvektor zur Liste hinzu\n",
    "    if word_vectors_list:\n",
    "        return np.mean(word_vectors_list, axis=0)  # Berechne den Durchschnitt der Vektoren\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)  # Wenn kein Wort im Vokabular ist, gebe einen Nullvektor zur√ºck\n",
    "\n",
    "# Optionale Verbesserung: Stoppw√∂rter entfernen (z. B. \"und\", \"der\", \"die\")\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('german'))\n",
    "# words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Optionale Verbesserung: Lemmatisierung anwenden\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Texte in ihre durchschnittlichen Wortvektoren konventieren\n",
    "train_df['text_vector'] = train_df['Text'].apply(lambda x: get_average_word_vector(x, word_vectors))\n",
    "test_df['text_vector'] = test_df['Text'].apply(lambda x: get_average_word_vector(x, word_vectors))\n",
    "\n",
    "# Verbesserung: Anstelle von Durchschnitt kannst du TF-IDF-Gewichtung der Vektoren ausprobieren\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf.fit_transform(train_df['Text'])\n",
    "\n",
    "# Daten f√ºr das Training vorbereiten\n",
    "X_train = np.vstack(train_df['text_vector'])  # die Vektoren in ein 2D-Array stapeln\n",
    "y_train = train_df['Label']  # Zielvariable (Labels) f√ºr das Training\n",
    "X_test = np.vstack(test_df['text_vector'])  # die Vektoren f√ºr die Testdaten stapeln\n",
    "y_test = test_df['Label']  # Zielvariable (Labels) f√ºr die Testdaten\n",
    "\n",
    "# Verbesserung: Cross-Validation verwenden, um die Generalisierung zu √ºberpr√ºfen\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "# print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Trainiere eine logistische Regression auf den durchschnittlichen Wortvektoren\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Verbesserung: Hyperparameteroptimierung f√ºr die logistische Regression\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear']}\n",
    "# grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# Vorhersagen machen und auswerten\n",
    "y_train_pred = model.predict(X_train)  # Vorhersagen f√ºr die Trainingsdaten machen\n",
    "y_test_pred = model.predict(X_test)    # Vorhersagen f√ºr die Testdaten machen\n",
    "\n",
    "train_accuracy_logreg = accuracy_score(y_train, y_train_pred)  # Berechne die Genauigkeit f√ºr die Trainingsdaten\n",
    "test_accuracy_logreg = accuracy_score(y_test, y_test_pred)     # Berechne die Genauigkeit f√ºr die Testdaten\n",
    "#Ausgabe Genauigkeit\n",
    "print(f\"Trainingsgenauigkeit: {train_accuracy_logreg:.4f}\")    # Ausgabe der Trainingsgenauigkeit\n",
    "print(f\"Testgenauigkeit: {test_accuracy_logreg:.4f}\")          # Ausgabe der Testgenauigkeit\n",
    "\n",
    "# Klassifikationsbericht f√ºr den Testdatensatz ausgeben\n",
    "print(\"Klassifikationsbericht f√ºr den Testdatensatz:\")\n",
    "print(classification_report(y_test, y_test_pred))       # Zeige den Bericht mit Pr√§zision, Recall und F1-Score an\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e130a747-ae67-47e0-a0c6-db7adbad443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34566     <class 'numpy.ndarray'>\n",
      "223092    <class 'numpy.ndarray'>\n",
      "110270    <class 'numpy.ndarray'>\n",
      "365013    <class 'numpy.ndarray'>\n",
      "311625    <class 'numpy.ndarray'>\n",
      "                   ...           \n",
      "243978    <class 'numpy.ndarray'>\n",
      "99503     <class 'numpy.ndarray'>\n",
      "184853    <class 'numpy.ndarray'>\n",
      "368110    <class 'numpy.ndarray'>\n",
      "225881    <class 'numpy.ndarray'>\n",
      "Name: text_vector, Length: 168000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Pr√ºft den Datentyp der Werte in der Spalte 'text_vector' des DataFrames train_df\n",
    "print(train_df['text_vector'].apply(lambda x: type(x)))\n",
    "\n",
    "# Konvertiert die Werte in der Spalte 'text_vector' des DataFrames train_df in den Datentyp float32\n",
    "train_df['text_vector'] = train_df['text_vector'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "# Konvertiert die Werte in der Spalte 'text_vector' des DataFrames test_df in den Datentyp float32\n",
    "test_df['text_vector'] = test_df['text_vector'].apply(lambda x: x.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252f2dad-9aa1-4b3d-aa15-8979df88c28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a6c4dcef9b4dfaa293e2432c1f6d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/168000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac4c75807164a858a08b5a7354fe81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21000' max='21000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21000/21000 1:04:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.165651</td>\n",
       "      <td>0.965702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Genauigkeit: 0.9657\n",
      "Klassifikationsbericht f√ºr das Test-Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      5685\n",
      "           1       0.97      0.96      0.97      5715\n",
      "\n",
      "    accuracy                           0.97     11400\n",
      "   macro avg       0.97      0.97      0.97     11400\n",
      "weighted avg       0.97      0.97      0.97     11400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Genauigkeit: 0.9764\n"
     ]
    }
   ],
   "source": [
    "# Benenne die Spalte 'Label' in 'labels' um, um dem erwarteten Format f√ºr Hugging Face Modelle zu entsprechen\n",
    "train_dataset = train_dataset.rename_column(\"Label\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"Label\", \"labels\")\n",
    "\n",
    "# Lade den Tokenizer f√ºr RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Funktion zum Tokenisieren der Datasets\n",
    "def tokenize_function(examples):\n",
    "    # Tokenisiert den Text mit Padding und Trunkierung, maximaler L√§nge von 256 Tokens\n",
    "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Optionale k√∂nnte die maximale L√§nge angepasst werden oder eine dynamische L√§nge ohne Padding verwenden werden\n",
    "# return tokenizer(examples['Text'], padding=True, truncation=True)\n",
    "\n",
    "# Tokenisiere das Trainings- und Test-Dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Die 'Text' Spalte entfernen, da sie nun nicht mehr ben√∂tigt wird\n",
    "train_dataset = train_dataset.remove_columns([\"Text\"])\n",
    "test_dataset = test_dataset.remove_columns([\"Text\"])\n",
    "\n",
    "# Stelle sicher, dass die Datasets im richtigen Format vorliegen\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Lade das RoBERTa Modell f√ºr Klassifikationsaufgaben\n",
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "# Verbesserung: Modell mit finetuning vorbereiten, zus√§tzliche Schichten hinzuf√ºgen \n",
    "# z.B. \"roberta-base\" oder \"roberta-large\"\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Training-Argumente einrichten\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Verzeichnis f√ºr die Ergebnisse des Trainings\n",
    "    evaluation_strategy=\"epoch\",  # Evaluierung nach jeder Epoche\n",
    "    learning_rate=2e-5,  # Lernrate (man k√∂nnte mit anderen Werten wie 3e-5, 5e-5 experimentieren)\n",
    "    per_device_train_batch_size=4,  # Kleinere Batch-Gr√∂√üe f√ºr das Training\n",
    "    per_device_eval_batch_size=4,   # Kleinere Batch-Gr√∂√üe f√ºr die Evaluation\n",
    "    num_train_epochs=1,  # Anzahl der Epochen f√ºr das Training (kann auch erh√∂ht werden, um die Leistung zu verbessern)\n",
    "    weight_decay=0.01,   # Gewichtungsverfall (Regularisierung)\n",
    "    logging_dir='./logs',  # Verzeichnis f√ºr die Log-Dateien\n",
    "    fp16=True,  # Mixed Precision Training f√ºr schnellere Berechnungen\n",
    "    gradient_accumulation_steps=2   # Gradient Accumulation, um eine gr√∂√üere Batch-Gr√∂√üe zu simulieren\n",
    ")\n",
    "\n",
    "# Verbesserung: Experimentieren mit gr√∂√üeren Batch-Gr√∂√üen und l√§ngeren Trainingsphasen\n",
    "# z.B. per_device_train_batch_size=8, num_train_epochs=3\n",
    "\n",
    "# Trainer definieren\n",
    "trainer = Trainer(\n",
    "    model=model,  # Das Modell, das trainiert werden soll\n",
    "    args=training_args,  # Die Trainingsargumente\n",
    "    train_dataset=train_dataset,  # Das Trainings-Dataset\n",
    "    eval_dataset=test_dataset,  # Das Evaluierungs-Dataset\n",
    "    compute_metrics=lambda p: {'accuracy': accuracy_score(p.predictions.argmax(axis=-1), p.label_ids)},  # Berechnung der Genauigkeit\n",
    ")\n",
    "\n",
    "# Bereinige den CUDA-Speicher, bevor das Training startet\n",
    "torch.cuda.empty_cache()\n",
    "# Model trainieren\n",
    "trainer.train()\n",
    "\n",
    "# Speichere das Model\n",
    "trainer.save_model(\"./roberta_sentiment_model\")\n",
    "\n",
    "# Modell auf die Testdaten anwenden\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_test_pred = predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# Berechne die Genauigkeit des Modells auf dem Test-Set\n",
    "test_accuracy_roberta = accuracy_score(test_df['Label'], y_test_pred)\n",
    "print(f\"Test Genauigkeit: {test_accuracy_roberta:.4f}\")\n",
    "\n",
    "# Drucke den Klassifikationsbericht f√ºr das Test-Set\n",
    "print(\"Klassifikationsbericht f√ºr das Test-Set:\")\n",
    "print(classification_report(test_df['Label'], y_test_pred))\n",
    "\n",
    "# Vorhersagen auf dem Trainings-Set durchf√ºhren, um die Trainingsgenauigkeit zu berechnen\n",
    "train_predictions = trainer.predict(train_dataset)\n",
    "y_train_pred = train_predictions.predictions.argmax(axis=-1)\n",
    "\n",
    "# Berechne die Genauigkeit des Modells auf dem Trainings-Set\n",
    "train_accuracy_roberta = accuracy_score(train_df['Label'], y_train_pred)\n",
    "print(f\"Training Genauigkeit: {train_accuracy_roberta:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15c32ff-8540-417e-8060-126bf887754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model  Training Accuracy  Test Accuracy\n",
      "0                  TextBlob Sentiment           0.315536       0.317456\n",
      "1  Logistic Regression (Word Vectors)           0.832452       0.831842\n",
      "2                       DistilRoBERTa           0.976363       0.965702\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    {\"Model\": \"TextBlob Sentiment\", \"Training Accuracy\": train_accuracy_textblob, \"Test Accuracy\": test_accuracy_textblob},\n",
    "    {\"Model\": \"Logistic Regression (Word Vectors)\", \"Training Accuracy\": train_accuracy_logreg, \"Test Accuracy\": test_accuracy_logreg},\n",
    "    {\"Model\": \"DistilRoBERTa\", \"Training Accuracy\": train_accuracy_roberta, \"Test Accuracy\": test_accuracy_roberta}\n",
    "]\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Tabelle ausgeben\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8105b5-f157-46fd-94ca-9f6166c6692b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea14560-ce09-444f-89a0-6d70b9dea092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
